{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e11b7a-5d2d-453e-82a8-6150173419df",
   "metadata": {},
   "source": [
    "# Web Scrapping for Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee304a9-84e5-49c3-a70c-048dd784df5e",
   "metadata": {},
   "source": [
    "## Part 1: Scraping Auction Catalog PDFs\n",
    "_____________________\n",
    "\n",
    "In this stage, I extract auction and painting information from multiple auction catalog PDFs. I have collected hundreds of samples across categories such as old masters and modern/contemporary art. PDF extraction is the preferred method, as auction platforms often employ highly dynamic , making direct web scraping impractical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a30758-e87e-4b34-aa2b-4dd031285d7e",
   "metadata": {},
   "source": [
    "### 1. Import Libraries\n",
    "Import the libraries needed for extracting data from pdf and formatting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3abbbe7-f475-4afa-b809-08feccfe9c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re # Regular Expressions for locating the string to be extract\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for extracting text from PDFs\n",
    "import unidecode # Format text to english readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c10a34-6e59-45fa-85b9-956742d9b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define folder path where PDFs are stored\n",
    "pdf_folder = \"../art_paintings/\"\n",
    "data_list = []  # Store extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5eae62-0c79-4f1f-81d4-bd17d3026061",
   "metadata": {},
   "source": [
    "### 2. Review Extracted Data from PDF\n",
    "Run PyMuPDF to see what data have been extracted from the multiple PDFs in local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376739b7-3e18-4e9d-bea1-682746e19c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each PDF in the folder\n",
    "# os. listdir() returns a list of the names of the entries in the directory\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"): # Only process PDFs\n",
    "        file_path = os.path.join(pdf_folder, filename) # Create path name by combining folder location and filename\n",
    "        \n",
    "        # Open PDF\n",
    "        doc = fitz.open(file_path)\n",
    "        print(f\"Extracting from: {filename}\") # Show which file is being processed\n",
    "        \n",
    "        # page_num gives the page number (starting from 0)\n",
    "        # enumerate(doc): Loops through each page in the PDF\n",
    "        for page_num, page in enumerate(doc):\n",
    "            text = page.get_text(\"text\") # Extracts text from the page\n",
    "            print(f\"Page {page_num + 1}:\\n{text}\\n\") # Display the page number and its text\n",
    "\n",
    "        print(\"-\" * 60) # Print a separator line after each PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a171d6-49ef-42c1-8b42-6a40f2cdd5e3",
   "metadata": {},
   "source": [
    "### 3. Extracting and Structuring Auction Data from PDFs\n",
    "\n",
    "This step involves extracting auction information from catalog PDFs using PyMuPDF and regular expressions (regex). Specific regex patterns are crafted to locate key data points such as the artist's name, birth year, title, and other relevant details. For each piece of information, the script searches the text using targeted patterns to accommodate the diverse formatting found across different PDFs. Once the data is extracted, it is compiled into a dictionary and then saved as a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0522ecf7-ed93-4067-91f0-168091a80d29",
   "metadata": {},
   "source": [
    "### 3a. Define the Function for Extracting Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e49c01-22d3-4e43-8a45-c2b008e85941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structured information from the text using regex\n",
    "def extract_info(text):\n",
    "    # setting up regex to location information within the pdf document\n",
    "    artist_match = re.search(r\"(?m)^\\d+\\n([^\\n]+)\", text, re.MULTILINE)\n",
    "    birth_year_match = re.search(r\"(?m)\\n^b\\.\\s*(\\d{4})\\b\", text, re.MULTILINE)\n",
    "    birth_death_year_match = re.search(r\"^(\\d{4} - \\d{4})$\", text, re.MULTILINE)\n",
    "    title_match = re.search(r\"^\\d+\\n[^\\n]+\\n([^\\n]+)\", text, re.MULTILINE)\n",
    "    year_match = re.search(r\"Executed (?:in|circa) (\\d{4})\", text)\n",
    "    medium_match = re.search(r\"(\\b(?:oil|ink|acrylic|tempera|watercolor|charcoal|graphite|mixed media|print|photograph)[^.\\n]+)\", text, re.IGNORECASE)\n",
    "    dimensions_in_match = re.search(r\"([\\d\\.]+ ?(?:by|x) ?[\\d\\.]+ in)\", text)\n",
    "    dimensions_cm_match = re.search(r\"([\\d\\.]+ ?(?:by|x) ?[\\d\\.]+ cm)\", text)\n",
    "    lot_number_match = re.search(r\"(?m)^\\s*(\\d+)\\n[^\\n]+\\n\", text)\n",
    "    estimate_match = re.search(r\"Estimate:\\s*(.*?)\\n\", text)\n",
    "    sold_price_match = re.search(r\"Lot Sold:\\s*(.*?)\\n\", text)\n",
    "    condition_matches = re.findall(r\"\\b(?:Condition Report|Condition:)\\s*\\n([^.\\n]+)\", text, re.IGNORECASE)\n",
    "\n",
    "    # Convert the artist's name and title to plain ASCII characters (removing accents)\n",
    "    artist_name = unidecode.unidecode(artist_match.group(1)) if artist_match else None\n",
    "    title = unidecode.unidecode(title_match.group(1)) if title_match else None\n",
    "\n",
    "    # Determine birth and death years:\n",
    "    # If a birth-death range is found, split it into birth and death years\n",
    "    # If only the birth year is found, use it and leave death year empty\n",
    "    if birth_death_year_match:\n",
    "        birth_year, death_year = birth_death_year_match.group(1).split(' - ')\n",
    "    elif birth_year_match:\n",
    "        birth_year = birth_year_match.group(1).strip()\n",
    "        death_year = None\n",
    "    else:\n",
    "        birth_year, death_year = None, None\n",
    "\n",
    "    # Combine multiple condition details into a single string, separated by commas\n",
    "    condition_report = \", \".join(condition_matches) if condition_matches else None\n",
    "\n",
    "    # Return all the extracted information as a dictionary\n",
    "    return {\n",
    "        \"Artist\": artist_name,\n",
    "        \"Birth_Year\": birth_year,\n",
    "        \"Death_Year\": death_year,\n",
    "        \"Title\": title,\n",
    "        \"Year_Created\": year_match.group(1) if year_match else None,\n",
    "        \"Medium\": medium_match.group(1) if medium_match else None,\n",
    "        \"Dimensions(in)\": dimensions_in_match.group(1) if dimensions_in_match else None,\n",
    "        \"Dimensions(cm)\": dimensions_cm_match.group(1) if dimensions_cm_match else None,\n",
    "        \"lot_number\": lot_number_match.group(1) if lot_number_match else None,\n",
    "        \"Estimate Price\": estimate_match.group(1) if estimate_match else None,\n",
    "        \"Final Sold Price\": sold_price_match.group(1) if sold_price_match else None,\n",
    "        \"Condition Report\": condition_report,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128f784-3ac3-4031-b7c4-fa128c565dd0",
   "metadata": {},
   "source": [
    "### 3b. Process Each PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db43c0-ffe8-4324-9d11-28099396a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file in the PDF folder\n",
    "for filename in os.listdir(pdf_folder):\n",
    "    if filename.endswith(\".pdf\"):  # Only process PDFs\n",
    "        file_path = os.path.join(pdf_folder, filename)  #  Create path name by combining folder location and filename\n",
    "        doc = fitz.open(file_path)  # Open the PDF document\n",
    "        \n",
    "        # Process pages in pairs (each painting's info spans two pages)\n",
    "        for page_num in range(0, len(doc), 2):  \n",
    "            page_text = \"\"\n",
    "\n",
    "            # Get text from the current page\n",
    "            page_text += doc[page_num].get_text(\"text\")\n",
    "\n",
    "            # If the next page exists, add its text too\n",
    "            if page_num + 1 < len(doc):\n",
    "                page_text += \" \" + doc[page_num + 1].get_text(\"text\")\n",
    "\n",
    "            # Extract structured information from the combined text by 'extract_info' function\n",
    "            extracted_data = extract_info(page_text)\n",
    "\n",
    "            # Save the filename with the extracted data for tracking\n",
    "            extracted_data[\"File\"] = filename  \n",
    "\n",
    "            # Append the extracted information to the data list\n",
    "            data_list.append(extracted_data)\n",
    "\n",
    "        doc.close()  # Close the PDF after processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdcf87f-6105-41e5-9ae0-6dbbb2574482",
   "metadata": {},
   "source": [
    "### 3c. Convert Data to DataFrame and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338dd6e-fe1f-4943-b5b6-12dd6263edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save dataframe to CSV\n",
    "df.to_csv(\"auction_data.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"Data saved to auction_data.csv\") # notification to indicate csv save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48189b52-18ff-4b0f-b79a-44ad32ad6767",
   "metadata": {},
   "source": [
    "## Part 2: Scrapping with BeautifulSoup and Selenium\n",
    "_____________________\n",
    "\n",
    "In this stage, I collect artist performance metrics from an online auction platform by combining the strengths of Selenium and BeautifulSoup. Selenium is used to dynamically load the website, ensuring that all JavaScript-rendered content—including critical performance metrics—is fully captured. Once the page is rendered, BeautifulSoup is employed to parse the HTML structure. Artist-specific URLs are reconstructed, key metrics are extracted, and the data is organized into a CSV file for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b0e3e5-9291-4ca5-a159-d9ac09c60595",
   "metadata": {},
   "source": [
    "### 1. Install and Import Libraries\n",
    "Install Selenium to load javascript, BeautifulSoup to scrap HTML and time for interval to load the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c52d55-0ec8-417f-a9e2-38e4b75abbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd548f94-0c3b-42e5-8418-f5e51ad8647d",
   "metadata": {},
   "source": [
    "### 2. Define Function to Construct URL\n",
    "Each artist’s information page URL is defined by their name appended to the platform’s base URL. Artist names collected from Part 1 were saved into a new artists.csv file, which is then used to construct URLs for automated data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f2e57-ac24-493f-b96e-24da0ff8c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_url(artist_name):\n",
    "    \n",
    "    # Convert the artist's name into a URL-friendly format\n",
    "    # For example, \"Yoshitomo Nara\" becomes \"yoshitomo-nara\"\n",
    "    if not artist_name or not isinstance(artist_name, str):\n",
    "        raise ValueError(\"Artist name must be a non-empty string.\")\n",
    "    \n",
    "    # Remove extra spaces and change to lowercase\n",
    "    artist_name = artist_name.strip().lower()\n",
    "    \n",
    "    # Remove special characters, keeping only letters, numbers, spaces, and hyphens\n",
    "    artist_slug = re.sub(r\"[^\\w\\s-]\", \"\", artist_name)\n",
    "    # Replace spaces (and extra hyphens) with a single hyphen\n",
    "    artist_slug = re.sub(r\"[\\s-]+\", \"-\", artist_slug)\n",
    "    \n",
    "    # Build the URL using the formatted artist name\n",
    "    url = f\"https://www.example.com/{artist_slug}/results\"\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8ffea-ba16-4bc7-a1df-8c620b8b0608",
   "metadata": {},
   "source": [
    "### 3. Function to Scrape Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedc606-bb63-4057-b51b-547db0c4b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape metrics from an artist's results page using Selenium to render JavaScript\n",
    "def get_artist_metrics(artist_url):\n",
    "    \n",
    "    # Set up Selenium with headless Chrome (runs without opening a browser window)\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\") # Run Chrome in the background\n",
    "    chrome_options.add_argument(\"--disable-gpu\") # Disable GPU use to avoid potential issues in headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\") # Turn off Chrome's sandbox for easier setup\n",
    "    driver = webdriver.Chrome(options=chrome_options) # load Chrome webdriver\n",
    "    \n",
    "    # Open the artist's page and wait for all the content to load\n",
    "    driver.get(artist_url)\n",
    "    time.sleep(5) # Wait 5 seconds for dynamic content to load fully\n",
    "    \n",
    "    # Get the full HTML of the loaded page\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    driver.quit()  # Close the browser once the page is loaded\n",
    "\n",
    "    # Set default values for the metrics\n",
    "    metrics = {\n",
    "        \"critically_acclaimed\": \"N/A\",\n",
    "        \"followers\": \"N/A\",\n",
    "        \"values\": \"N/A\",\n",
    "    }\n",
    "    \n",
    "    # Look for buttons that say \"Critically acclaimed\"\n",
    "    crit_buttons = soup.find_all(\"button\", class_=\"Clickable-sc-10cr82y-0 fbnHxf\")\n",
    "    metrics[\"critically_acclaimed\"] = \"No\"\n",
    "    # If any button shows \"Critically acclaimed\", mark it as \"Yes\"\n",
    "    for btn in crit_buttons:\n",
    "        if \"Critically acclaimed\" in btn.get_text(strip=True):\n",
    "            metrics[\"critically_acclaimed\"] = \"Yes\"\n",
    "            break\n",
    "\n",
    "    # Look for the followers count in the designated div\n",
    "    follower_div = soup.find(\"div\", class_=\"Box-sc-15se88d-0 Text-sc-18gcpao-0 cZekcQ gviZDz\")\n",
    "    # If found, extract its value and save it as the followers metric.\n",
    "    if follower_div:\n",
    "        metrics[\"followers\"] = follower_div.get_text(strip=True)\n",
    "    \n",
    "    # Find the section that holds other metric values\n",
    "    value_elements = soup.find(\"div\", class_=\"Box-sc-15se88d-0 CSSGrid-sc-1q8w5xn-0 GridColumns-sc-1g9p6xx-0 jdZUdM gRoBRz fwdhTL\")\n",
    "    # If found, extract its text and save it as the values metric\n",
    "    if value_elements:\n",
    "        if value_elements:\n",
    "            metrics[\"values\"] = value_elements.get_text(strip=True)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f927a2-9d80-4392-a4bc-c7552f9198d7",
   "metadata": {},
   "source": [
    "### 4. Load CSV File of Artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a362772e-1d77-4589-891e-8d8146b07624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the CSV file that contains a list of artists\n",
    "try:\n",
    "    artists_df = pd.read_csv(\"artists.csv\")\n",
    "    # Check if the CSV has an \"artist\" column. If not, throw an error\n",
    "    if \"artist\" not in artists_df.columns:\n",
    "        raise ValueError(\"CSV file must contain an 'artist' column.\")\n",
    "except Exception as e:\n",
    "    # Print any error that occurs during loading and stop the script\n",
    "    print(f\"Error loading CSV file: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc31d88-daec-48be-b866-82f4c7f07aa4",
   "metadata": {},
   "source": [
    "### 5. Scrape Data for Each Artist\n",
    "\n",
    "An artists.csv file is loaded into Python to extract artist names. These names are then used to construct URLs for scraping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000ad34-fbfd-49b0-9b44-ecc8b8e441fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare an empty list to store data that we will scrape\n",
    "data_list = []\n",
    "\n",
    "# Loop over each row (artist) in the CSV\n",
    "for index, row in artists_df.iterrows():\n",
    "    artist_name = row[\"artist\"]\n",
    "    # Build the URL for the artist page\n",
    "    url = construct_url(artist_name)\n",
    "    print(f\"Scraping data for {artist_name} from {url}\")\n",
    "\n",
    "    # Get the artist's metrics by scraping the page\n",
    "    metrics = get_artist_metrics(url)\n",
    "    if metrics:\n",
    "        # Add the artist's name and URL to the metrics data\n",
    "        metrics[\"artist\"] = artist_name\n",
    "        metrics[\"url\"] = url\n",
    "        # Save this data into our list\n",
    "        data_list.append(metrics)\n",
    "    \n",
    "    time.sleep(1)  # Wait for 1 second between requests to avoid overwhelming the server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bea46-ba10-40e7-876a-c377716f09b9",
   "metadata": {},
   "source": [
    "### 6. Save Scraped Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8c526-c938-4d4e-861e-d7ac37458542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the scraped data\n",
    "scraped_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the scraped metrics to a CSV file\n",
    "scraped_df.to_csv(\"artists_scraped_metrics.csv\", index=False)\n",
    "print(\"Scraping complete. Data saved to artists_scraped_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
